{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72eadf46",
   "metadata": {},
   "source": [
    "### This script is created by Hugsney\n",
    "<p>Automatizacao do processo de pagamento</p>\n",
    "<br>\n",
    "<h1>Primeira etapa do script - Lendo Banco de dados</h1>\n",
    "<h6>celula_1</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12f9115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Arquivo \"banco de dados\" e \"Outras Informações\" carregados com sucesso\n"
     ]
    }
   ],
   "source": [
    "# Livrarias importadas\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import xlsxwriter\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib import colors\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import zipfile\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "\n",
    "# Criando uma variavel com a data atualizada\n",
    "data = datetime.now().strftime('%d-%m-%Y')\n",
    "data_arquivo = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "\n",
    "# Data de ontem\n",
    "#ontem = datetime.now() - timedelta(days=1)\n",
    "#data = ontem.strftime('%d-%m-%Y')        # Ex: '30-03-2025'\n",
    "#data_arquivo = ontem.strftime('%Y%m%d')  # Ex: '20250330'\n",
    "\n",
    "\n",
    "# Definir o caminho para os arquivos serem encontrados\n",
    "folder_path = \"/Users/hugsneyf/Downloads/outros arquivos/project_Gui_Uber/Historico\"\n",
    "arquivos_semanais = '/Users/hugsneyf/Downloads/outros arquivos/project_Gui_Uber/Arquivos_Bolt_Uber'\n",
    "\n",
    "# Caminho da nova pasta\n",
    "new_folder_path = os.path.join(folder_path, f\"{data_arquivo}\")\n",
    "if not os.path.exists(new_folder_path):\n",
    "    os.makedirs(new_folder_path)\n",
    "\n",
    "def save_df_to_excel(data_frame, file_name, nome_aba='Sheet1'):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to Excel, adjusting the width of the columns.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_frame: The DataFrame to be saved.\n",
    "    - file_name: The name the file will be saved as.\n",
    "    - sheet_name: Name of the sheet in the Excel file.\n",
    "    \"\"\"\n",
    "\n",
    "        # Create a folder using the current date\n",
    "    folder_with_date = os.path.join(new_folder_path)\n",
    "    os.makedirs(folder_with_date, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "    \n",
    "    # Define the full file path\n",
    "    full_file_path = os.path.join(folder_with_date, file_name)\n",
    "\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "    writer = pd.ExcelWriter(full_file_path, engine='xlsxwriter')\n",
    "    \n",
    "    # Convert the DataFrame to an XlsxWriter Excel object.\n",
    "    data_frame.to_excel(writer, sheet_name=nome_aba, index=False)\n",
    "    \n",
    "    # Get the xlsxwriter workbook and worksheet objects.\n",
    "    workbook  = writer.book\n",
    "    worksheet = writer.sheets[nome_aba]\n",
    "    \n",
    "    # Loop through each column and set the width based on the max length in that column.\n",
    "    # A little extra width is added for padding.\n",
    "    for i, col in enumerate(data_frame.columns):\n",
    "        col_str = str(col)\n",
    "        column_len = data_frame[col].astype(str).str.len().max()\n",
    "        column_len = max(column_len, len(col_str)) + 2  # Adding a little extra space\n",
    "        worksheet.set_column(i, i, column_len)\n",
    "    \n",
    "    # Close the Pandas Excel writer and output the Excel file.\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "#------------------------------------------ Loading the files  ------------------------------------------\n",
    "\n",
    "###################################  Arquivos de Banco de Dados  #####################################\n",
    "\n",
    "# Criando variavel do banco de dados\n",
    "banco_de_dados = pd.read_excel(f'Banco_de_Dados_Guilherme.xlsx')\n",
    "banco_de_dados['UUID do motorista'] = banco_de_dados['UUID do motorista'].astype(str)\n",
    "\n",
    "# Replace 'Banco_de_Dados_Guilherme.xlsx' with the path to your Excel file\n",
    "outras_info = pd.read_excel('Banco_de_Dados_Guilherme.xlsx', \n",
    "                            sheet_name='Outras informações')\n",
    "outras_info['NOME'] = outras_info['NOME'].str.title()\n",
    "\n",
    "\n",
    "# Extract required columns from Excel\n",
    "nome_arquivo = outras_info['Novo Nome'].dropna().tolist()\n",
    "origem_ref = outras_info['Origem_Ref'].dropna().tolist()\n",
    "uuid_driver = [int(float(x)) for x in outras_info['Uuid_driver'].dropna().tolist()]\n",
    "first_name = [int(float(x)) for x in outras_info['First_Name'].dropna().tolist()]\n",
    "last_name = [int(float(x)) for x in outras_info['Last_Name'].dropna().tolist()]\n",
    "salario_bruto = [int(float(x)) for x in outras_info['Valor_bruto'].dropna().tolist()]\n",
    "\n",
    "print('\\n- Arquivo \"banco de dados\" e \"Outras Informações\" carregados com sucesso')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a65d48c",
   "metadata": {},
   "source": [
    "<h1>Juntar arquivos Uber e Bolt, para identificar novos drives</h1>\n",
    "<h6>celula_2</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1dd47bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Alerta: Não foi encontrado o arquivo: ['PORTALEGRE'].\"*\n",
      "\n",
      " - Foram adicionados os arquivos: ['ACOURES', 'AVEIRO', 'ALGARVE', 'BEJA', 'BOLT DRIVER WINGS', 'BOLT DRIVER WORK', 'BOLT FOOD', 'BRAGA', 'BRAGANCA', 'CASTELO BRANCO', 'COIMBRA', 'EVORA', 'LEIRIA', 'LISBOA BIKE', 'LISBOA MOTO', 'LISBOA RESERVA', 'PEDRO BRAGA', 'PEDRO PORTO', 'PORTO', 'SANTAREM', 'TVDE NICOLY NOVO', 'TVDE VINICIO NOVO', 'VIANA DO CASTELO', 'VILA REAL', 'VISEU'].\n",
      "Duplicated UUIDs: 0\n",
      "\n",
      "\n",
      "*** Abra o arquivo: 'drivers_sem_registro': 64 registro/s para ser adicionados ao Banco de Dados ***\n",
      "\n",
      "\n",
      "            Resumo:\n",
      "                   Arq_origem  num_registros\n",
      "          ALGARVE              2\n",
      "           AVEIRO              1\n",
      "BOLT DRIVER WINGS              1\n",
      " BOLT DRIVER WORK              1\n",
      "        BOLT FOOD             27\n",
      "            BRAGA              3\n",
      "            EVORA              2\n",
      "      LISBOA BIKE              3\n",
      "      LISBOA MOTO              3\n",
      "   LISBOA RESERVA              9\n",
      "            PORTO              1\n",
      " TVDE NICOLY NOVO              6\n",
      "TVDE VINICIO NOVO              3\n",
      " VIANA DO CASTELO              1\n",
      "        VILA REAL              1\n",
      "          \n",
      "\n",
      "\n",
      "\n",
      " - Existe um total de 1620 pagamentos a serem feitos.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################################  Arquivo consolidado Uber / Bolt #####################################\n",
    "\n",
    "# Criar o frame vazio para consolidar os arquivos de pagamento\n",
    "para_pagamento = []\n",
    "arquivos_carregados = []\n",
    "arquivos_faltando = []\n",
    "\n",
    "# Validar todos os arquivos\n",
    "for arquivo, origem, first, last, uuid, salario in zip(nome_arquivo, origem_ref, \n",
    "                                                       first_name, last_name, \n",
    "                                                       uuid_driver, salario_bruto):\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        arquivos_semanais_temp = os.path.join(arquivos_semanais, f\"{arquivo}.csv\")\n",
    "        \n",
    "        file = pd.read_csv(arquivos_semanais_temp, on_bad_lines='skip')\n",
    "\n",
    "        # Keep only the specified columns and rename them\n",
    "        columns_to_keep = [file.columns[uuid], file.columns[first], \n",
    "                           file.columns[last], file.columns[salario]]\n",
    "        file = file[columns_to_keep]\n",
    "        file.columns = ['UUID do motorista', 'first', 'last', 'Salario Bruto']\n",
    "\n",
    "        # Add a new column 'nome_arquivo' with the value of 'arquivo'\n",
    "        file['Arq_origem'] = arquivo\n",
    "        file['Origem_Ref'] = origem\n",
    "        \n",
    "        # Adiciona a list de nome de arquivos\n",
    "        arquivos_carregados.append(arquivo)\n",
    "        \n",
    "        # Append the modified DataFrame to para_pagamento list\n",
    "        para_pagamento.append(file)\n",
    "    \n",
    "    # Verifica arquivo faltando\n",
    "    except FileNotFoundError:        \n",
    "        arquivos_faltando.append(arquivo)\n",
    "\n",
    "\n",
    "# Printa mensagem sobre arquivos faltantes\n",
    "if len(arquivos_faltando) > 1:\n",
    "    print(f'\\n* Alerta: Não foram encontrados os arquivos: {arquivos_faltando}.\"*')\n",
    "elif len(arquivos_faltando) == 1:\n",
    "    print(f'\\n* Alerta: Não foi encontrado o arquivo: {arquivos_faltando}.\"*')\n",
    "\n",
    "# Printa mensagem sobre arquivos combinados\n",
    "if len(arquivos_carregados) > 1:\n",
    "    print(f'\\n - Foram adicionados os arquivos: {arquivos_carregados}.')\n",
    "elif len(arquivos_carregados) < 1:\n",
    "    print(f'\\n Nao Foram encontrados nenhum arquivo para pagamento')\n",
    "\n",
    "    \n",
    "# Concatenate all DataFrames in para_pagamento into a single DataFrame\n",
    "consolidado_pgto = pd.concat(para_pagamento)\n",
    "consolidado_pgto['UUID do motorista'] = consolidado_pgto['UUID do motorista'].astype(str)\n",
    "consolidado_pgto['UUID do motorista'] = consolidado_pgto['UUID do motorista'].str.replace('+', '')\n",
    "consolidado_pgto = consolidado_pgto[consolidado_pgto['UUID do motorista'] != 'nan']\n",
    "\n",
    "consolidado_pgto['UUID do motorista'] = consolidado_pgto['UUID do motorista'].str.strip()\n",
    "\n",
    "\n",
    "\n",
    "# Clean Salario Bruto\n",
    "consolidado_pgto['Salario Bruto'] = (consolidado_pgto['Salario Bruto'].astype(str).str.replace(',', '.', regex=False))\n",
    "consolidado_pgto['Salario Bruto'] = pd.to_numeric(consolidado_pgto['Salario Bruto'], errors='coerce')\n",
    "consolidado_pgto = consolidado_pgto[consolidado_pgto['Salario Bruto'] > 0]\n",
    "\n",
    "# Group\n",
    "consolidado_pgto = consolidado_pgto.groupby('UUID do motorista', as_index=False).agg({\n",
    "    'Salario Bruto': 'sum',\n",
    "    'first': 'first',\n",
    "    'last': 'first',\n",
    "    'Arq_origem': 'first',\n",
    "    'Origem_Ref': 'first'\n",
    "})\n",
    "\n",
    "\n",
    "dupes = consolidado_pgto['UUID do motorista'].duplicated(keep=False).sum()\n",
    "print(f\"Duplicated UUIDs: {dupes}\")\n",
    "\n",
    "\n",
    "# Isola drivers da uber sem registro\n",
    "driver_sem_registro = consolidado_pgto[~consolidado_pgto['UUID do motorista'].isin(banco_de_dados['UUID do motorista'])]\n",
    "driver_sem_registro_size = len(driver_sem_registro['UUID do motorista'])\n",
    "\n",
    "# Checa o tamanho do arquivo, se tiver algo cria um novo arquivo\n",
    "if driver_sem_registro_size > 0:\n",
    "\n",
    "    # Excuta a função previaviamente criada para salvar o arquivo\n",
    "    #save_df_to_excel(driver_sem_registro, f\"{data_arquivo}_drivers_sem_registro - DELETE ME.xlsx\", \n",
    "    #                 'driver_sem_registro_tab')\n",
    "    \n",
    "    # Resumir numeros de registros\n",
    "    num_registros = driver_sem_registro.groupby('Arq_origem').size().reset_index(name='num_registros')\n",
    "    num_registros = num_registros.to_string(index=False)\n",
    "\n",
    "    \n",
    "    # Display the result\n",
    "    print(f\"\"\"\\n\\n*** Abra o arquivo: 'drivers_sem_registro': {driver_sem_registro_size} registro/s para ser adicionados ao Banco de Dados ***\\n\\n\n",
    "            Resumo:\n",
    "            {num_registros}\n",
    "          \\n\\n\"\"\")\n",
    "    \n",
    "    print(f'\\n - Existe um total de {len(consolidado_pgto)} pagamentos a serem feitos.\\n\\n')\n",
    "else:\n",
    "    print(f\"\\n- Sem novos Drivers para serem registrados.\")\n",
    "\n",
    "    \n",
    "# Group by the 'Arq_origem' column\n",
    "consolidado_pgto_resumo = consolidado_pgto.groupby('Arq_origem').size().reset_index(name='Count')\n",
    "\n",
    "# Print the summary table\n",
    "#print(consolidado_pgto_resumo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324089d",
   "metadata": {},
   "source": [
    "# Planilha geral\n",
    "<h6>celula_3</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f94f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verique o arquivo 'valores negativos' com os valores que nao irao ao arquivo para pagamento.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/3ylp0mwj1c37m5vjr6slv2qc0000gn/T/ipykernel_19816/1900742970.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  planilha_geral = pd.merge(consolidado_pgto, banco_de_dados, on=\"UUID do motorista\", how=\"left\").fillna(0)\n",
      "/var/folders/vr/3ylp0mwj1c37m5vjr6slv2qc0000gn/T/ipykernel_19816/1900742970.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  com_aluguel['key'] = com_aluguel['IBAN'].astype(str) + com_aluguel['DESCONTO ALUGUEL SEMANA'].astype(str)\n",
      "/var/folders/vr/3ylp0mwj1c37m5vjr6slv2qc0000gn/T/ipykernel_19816/1900742970.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  com_aluguel['is_duplicate'] = com_aluguel.duplicated('key', keep=False)\n",
      "/var/folders/vr/3ylp0mwj1c37m5vjr6slv2qc0000gn/T/ipykernel_19816/1900742970.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  com_aluguel_dup_uber['DESCONTO ALUGUEL SEMANA'] = 0\n"
     ]
    }
   ],
   "source": [
    "# Organizando os dados e renomeando as colunas da panilha - uber\n",
    "planilha_geral = pd.merge(consolidado_pgto, banco_de_dados, on=\"UUID do motorista\", how=\"left\").fillna(0)\n",
    "\n",
    "# Adiciona uma coluna com a data\n",
    "planilha_geral['Data'] = pd.to_datetime(data, format='%d-%m-%Y')\n",
    "\n",
    "# Arruma os tipos de dados das colunas numericas\n",
    "type_float_columns = ['Salario Bruto', 'DESCONTO ALUGUEL SEMANA', 'DESC. TRANSF BANCARIA', 'Deve(+)']\n",
    "\n",
    "# Remove virgulas e adiciona '.'\n",
    "planilha_geral[type_float_columns] = planilha_geral[type_float_columns].replace({',': '.'}, regex=True)\n",
    "\n",
    "# Troca qualquer texto por zero\n",
    "planilha_geral[type_float_columns] = planilha_geral[type_float_columns].replace({'[^0-9.]': '0'}, regex=True)\n",
    "\n",
    "# Garante o tipo de dados numerico\n",
    "planilha_geral[type_float_columns] = planilha_geral[type_float_columns].astype(float)\n",
    "\n",
    "\n",
    "# ----------------------------------------- Analises ----------------------------------------- #\n",
    "# Do grupo de pessoas com aluguel foram separadas 2 grupos:\n",
    "# 1- Todos que possuem desconto de aluguel\n",
    "com_aluguel = planilha_geral[planilha_geral['DESCONTO ALUGUEL SEMANA'] > 0]\n",
    "\n",
    "# 1- Cria uma chave para verificar os duplicados\n",
    "com_aluguel['key'] = com_aluguel['IBAN'].astype(str) + com_aluguel['DESCONTO ALUGUEL SEMANA'].astype(str)\n",
    "\n",
    "# 1- Mark all duplicates (first and subsequent occurrences) in the 'key' column\n",
    "com_aluguel['is_duplicate'] = com_aluguel.duplicated('key', keep=False)\n",
    "\n",
    "# 1.1 - Pessoas com aluguel em duplicidade e que esta marcada com Bolt --> essas foram zerados os descontos\n",
    "com_aluguel_dup_uber = com_aluguel[(com_aluguel['is_duplicate'] == True) & (com_aluguel['Plataforma'] == 'Bolt')]\n",
    "com_aluguel_dup_uber['DESCONTO ALUGUEL SEMANA'] = 0\n",
    "\n",
    "\n",
    "# 1.2.3 - Restante de pessoas com aluguel (SEM DIPLICIDADE E Uber COMO ORIGEM)\n",
    "com_aluguel_rest = com_aluguel[~((com_aluguel['is_duplicate'] == True) & (com_aluguel['Plataforma'] == 'Bolt'))]\n",
    "#com_aluguel_dup_rest = com_aluguel[(com_aluguel['is_duplicate'] == True) & (com_aluguel['Plataforma'] != 'Uber')]\n",
    "#com_aluguel_unique = com_aluguel[(com_aluguel['is_duplicate'] != True)]\n",
    "\n",
    "# 1- Juntando arquivos de pessoas com alugueis para os devidos calculos\n",
    "com_aluguel_resolvido = pd.concat([com_aluguel_dup_uber, com_aluguel_rest])\n",
    "\n",
    "# 1- Limpando arquivo de colunas extras\n",
    "com_aluguel_resolvido = com_aluguel_resolvido.drop(['is_duplicate', 'key'], axis=1)\n",
    "\n",
    "# 2- Pessoas sem desconto de aluguel\n",
    "sem_aluguel = planilha_geral[planilha_geral['DESCONTO ALUGUEL SEMANA'] <= 0]\n",
    "\n",
    "# Recriando arquivo final: planilha geral\n",
    "planilha_geral = pd.concat([sem_aluguel, com_aluguel_resolvido])\n",
    "\n",
    "\n",
    "# Adiciona coluna com a comissao geral, e quando nao ha registrando calcula com 10%\n",
    "planilha_geral['Comissao_Geral'] = planilha_geral.apply(\n",
    "    lambda row: (row['Salario Bruto'] * row['Percentual']) if row['Percentual'] > 0 \n",
    "    else (row['Salario Bruto'] * 0.10), axis=1).round(2)\n",
    "\n",
    "# Adiciona coluna com o valor final para pagamento\n",
    "planilha_geral['Valor a Pagar'] = (planilha_geral['Salario Bruto'] - # ele eh o salario bruto - a comissao\n",
    "                                       planilha_geral['Comissao_Geral'] -    #  - o desconto aluguel semana\n",
    "                                       planilha_geral['DESC. TRANSF BANCARIA'] -    #  - o desconto imeadiata\n",
    "                                       planilha_geral['DESCONTO ALUGUEL SEMANA'] + # e soma com algo que vc dever\n",
    "                                       planilha_geral['Deve(+)']).round(2) \n",
    "\n",
    "# Analise de valores negativos\n",
    "valores_negativos = planilha_geral[planilha_geral['Valor a Pagar'] < 0]\n",
    "\n",
    "save_df_to_excel(valores_negativos, f\"{data_arquivo}_valores_negativos.xlsx\", nome_aba='Sheet1')\n",
    "#save_df_to_excel(planilha_geral, f\"{data_arquivo}_planilha_geral.xlsx\", nome_aba='Sheet1')\n",
    "\n",
    "# Filtrando valores positivos\n",
    "planilha_geral = planilha_geral[planilha_geral['Valor a Pagar'].astype(float) > 0]\n",
    "\n",
    "# Arrumar nomes inseridos\n",
    "for col in ['Socio_1', 'Socio_2', 'Socio_3']:\n",
    "    planilha_geral[col] = planilha_geral[col].str.strip().str.title()\n",
    "\n",
    "# Adiciona coluna com o valor da comissão a receber\n",
    "planilha_geral['Valor Comissão'] = (planilha_geral['Comissao_Geral'] +\n",
    "                                       planilha_geral['DESCONTO ALUGUEL SEMANA']).round(2)\n",
    "\n",
    "def process_commission_column(df, banco_comissao, lucro_comission):\n",
    "    \"\"\"Função que arruma erros para cálculos de participação no lucro\"\"\"\n",
    "    \n",
    "    # Substitui valores não numéricos por '0' na coluna especificada\n",
    "    df[banco_comissao] = pd.to_numeric(df[banco_comissao], errors='coerce').fillna(0)\n",
    "\n",
    "    # Calcula a comissão com base no produto de 'Comissao_Geral' e a coluna de comissão especificada\n",
    "    df[lucro_comission] = (df['Comissao_Geral'] * df[banco_comissao]).round(2)\n",
    "\n",
    "# Criando colunas da participação dos lucros:\n",
    "process_commission_column(planilha_geral, 'Comissao_1', 'Comission_1')\n",
    "process_commission_column(planilha_geral, 'Comissao_2', 'Comission_2')\n",
    "process_commission_column(planilha_geral, 'Comissao_3', 'Comission_3')\n",
    "\n",
    "#save_df_to_excel(planilha_geral, f\"{data_arquivo}_planilha_geral_2.xlsx\", nome_aba='Sheet1')\n",
    "\n",
    "\n",
    "print(\"\\nVerique o arquivo 'valores negativos' com os valores que nao irao ao arquivo para pagamento.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f125d3",
   "metadata": {},
   "source": [
    "<h6>celula_4 - relatorios</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abb5bda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Planilha geral de dados e relatorio criados com sucesso.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/3ylp0mwj1c37m5vjr6slv2qc0000gn/T/ipykernel_19816/3237209316.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  relatorio_geral['NOME NOVO'] = relatorio_geral['first'] + ' ' + relatorio_geral['last']\n",
      "/var/folders/vr/3ylp0mwj1c37m5vjr6slv2qc0000gn/T/ipykernel_19816/3237209316.py:31: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top_5 = relatorio_geral.groupby('Arq_origem', group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Origem_Ref</th>\n",
       "      <th>Salario Bruto</th>\n",
       "      <th>Comissao_Geral</th>\n",
       "      <th>DESCONTO ALUGUEL SEMANA</th>\n",
       "      <th>Deve(+)</th>\n",
       "      <th>DESC. TRANSF BANCARIA</th>\n",
       "      <th>Valor a Pagar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bolt</td>\n",
       "      <td>30654.21</td>\n",
       "      <td>3015.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>771.2</td>\n",
       "      <td>26867.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Uber</td>\n",
       "      <td>210244.55</td>\n",
       "      <td>20701.36</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1692.8</td>\n",
       "      <td>187825.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Origem_Ref  Salario Bruto  Comissao_Geral  DESCONTO ALUGUEL SEMANA  Deve(+)  \\\n",
       "0       Bolt       30654.21         3015.72                      0.0      0.0   \n",
       "1       Uber      210244.55        20701.36                     25.0      0.0   \n",
       "\n",
       "   DESC. TRANSF BANCARIA  Valor a Pagar  \n",
       "0                  771.2       26867.29  \n",
       "1                 1692.8      187825.39  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## ---------------------------- RELATORIO 1 - Começo --------------------------------- ###########\n",
    "# Selecinando as colunas para o relatorio\n",
    "relatorio_geral_columns = ['Arq_origem', 'Origem_Ref', 'UUID do motorista', 'first', 'last', 'IBAN', 'Salario Bruto', 'Percentual', 'Valor a Pagar', 'Socio_2', 'Socio_3', 'Valor Comissão']\n",
    "relatorio_geral = planilha_geral[relatorio_geral_columns]\n",
    "\n",
    "# Criar a coluna 'NOME NOVO' concatenando 'first' e 'last' com um espaço entre eles\n",
    "relatorio_geral['NOME NOVO'] = relatorio_geral['first'] + ' ' + relatorio_geral['last']\n",
    "\n",
    "# Remover as colunas 'first' e 'last'\n",
    "relatorio_geral = relatorio_geral.drop(columns=['first', 'last'])\n",
    "\n",
    "# Calculate sums for specific columns\n",
    "total_row = {\n",
    "    'UUID do motorista': 'Total',\n",
    "    'NOME NOVO': '-',\n",
    "    'Salario Bruto': relatorio_geral['Salario Bruto'].sum(),\n",
    "    'Valor a Pagar': relatorio_geral['Valor a Pagar'].sum(),\n",
    "    'Valor Comissão': relatorio_geral['Valor Comissão'].sum()\n",
    "}\n",
    "\n",
    "# Append the total row to the DataFrame\n",
    "relatorio_geral = pd.concat([relatorio_geral, pd.DataFrame([total_row])], ignore_index=True)\n",
    "\n",
    "save_df_to_excel(relatorio_geral, f\"{data_arquivo}_relatorio_geral.xlsx\", nome_aba='Sheet1')\n",
    "######## ---------------------------- RELATORIO 1  -  Fim  --------------------------------- ###########\n",
    "\n",
    "\n",
    "######## ---------------------------- RELATORIO 1.1 - Top drivers --------------------------------- ###########\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "top_5 = relatorio_geral.groupby('Arq_origem', group_keys=False).apply(\n",
    "    lambda x: x.nlargest(5, 'Salario Bruto')\n",
    ")\n",
    "\n",
    "#top_20 = top_20[~top_20['UUID do motorista'].astype(str).str.contains('Total')]\n",
    "\n",
    "\n",
    "save_df_to_excel(top_5, f\"{data_arquivo}_top_5.xlsx\", nome_aba='Sheet1')\n",
    "\n",
    "######## ---------------------------- RELATORIO 1.1 - Top drivers  --------------------------------- ###########\n",
    "\n",
    "\n",
    "######## ---------------------------- RELATORIO 2 - LOCAL - Começo --------------------------------- ###########\n",
    "### Relatorio de Analises Comissoes Local\n",
    "# Ensure 'Valor Comissão' is numeric\n",
    "planilha_geral['Valor Comissão'] = pd.to_numeric(planilha_geral['Valor Comissão'], errors='coerce')\n",
    "planilha_geral[['Socio_1', 'Socio_2']] = planilha_geral[['Socio_1', 'Socio_2']].fillna('-')\n",
    "planilha_geral['Socio_3'] = planilha_geral['Socio_3'].fillna('NOVO PESSOAL')\n",
    "\n",
    "# Copia valores de uma coluna para a outra\n",
    "planilha_geral.loc[planilha_geral['Socio_1'] == '-', 'Comission_1'] = planilha_geral['Valor Comissão']\n",
    "\n",
    "# Criando tabelas dinamicas para consolidar as informacoes de comissao\n",
    "pivot_comisson1 = planilha_geral.pivot_table(index='Socio_1', columns='Arq_origem', values='Comission_1', aggfunc='sum', fill_value=0).reset_index().rename(columns={'Socio_1': 'Socio'})\n",
    "pivot_comisson2 = planilha_geral.pivot_table(index='Socio_2', columns='Arq_origem', values='Comission_2', aggfunc='sum', fill_value=0).reset_index().rename(columns={'Socio_2': 'Socio'})\n",
    "pivot_comisson3 = planilha_geral.pivot_table(index='Socio_3', columns='Arq_origem', values='Comission_3', aggfunc='sum', fill_value=0).reset_index().rename(columns={'Socio_3': 'Socio'})\n",
    "\n",
    "# Concatenate all three DataFrames vertically\n",
    "relatorio_comissao_local_grouped = pd.concat([pivot_comisson1, pivot_comisson2, pivot_comisson3], ignore_index=True)\n",
    "\n",
    "# Update name of the new drivers\n",
    "relatorio_comissao_local_grouped['Socio'] = relatorio_comissao_local_grouped['Socio'].replace('-', 'NOVO PESSOAL')\n",
    "\n",
    "# Agrupar por 'Socio' e somar as colunas numéricas para conseguir organizar o arquivo\n",
    "relatorio_comissao_local = relatorio_comissao_local_grouped.groupby('Socio').sum().reset_index()\n",
    "\n",
    "# Separando linhas principais para o top\n",
    "priority_names = [\"Guilherme\", \"NOVO PESSOAL\", \"Vinicio\"]\n",
    "\n",
    "# Separate rows that match the priority names\n",
    "priority_rows = relatorio_comissao_local[relatorio_comissao_local['Socio'].isin(priority_names)]\n",
    "\n",
    "# Separate the remaining rows\n",
    "remaining_rows = relatorio_comissao_local[~relatorio_comissao_local['Socio'].isin(priority_names)]\n",
    "\n",
    "# Sort the remaining rows as needed (e.g., alphabetically or by another column)\n",
    "remaining_rows = remaining_rows.sort_values(by=['Socio'])\n",
    "\n",
    "# Concatenate priority rows on top of sorted remaining rows\n",
    "relatorio_comissao_local = pd.concat([priority_rows, remaining_rows], ignore_index=True)\n",
    "\n",
    "# Add a column 'Total' summing all numeric columns horizontally (excluding 'Socio')\n",
    "numeric_cols = relatorio_comissao_local.columns.drop('Socio')\n",
    "relatorio_comissao_local['Total'] = relatorio_comissao_local[numeric_cols].sum(axis=1)\n",
    "\n",
    "# Ensure all numeric columns are correctly identified\n",
    "numeric_cols = relatorio_comissao_local.select_dtypes(include='number').columns\n",
    "\n",
    "# Compute the sum of numeric columns\n",
    "totals = relatorio_comissao_local[numeric_cols].sum()\n",
    "\n",
    "# Create a new row as a DataFrame\n",
    "total_row = pd.DataFrame([['Total Geral'] + totals.tolist()], columns=['Socio'] + list(numeric_cols))\n",
    "\n",
    "# Append the total row to the original DataFrame\n",
    "relatorio_comissao_local = pd.concat([relatorio_comissao_local, total_row], ignore_index=True)\n",
    "\n",
    "# Filter above zero\n",
    "relatorio_comissao_local = relatorio_comissao_local[relatorio_comissao_local['Total'] > 0]\n",
    "\n",
    "\n",
    "########################################### Salario total Tabela ###########################################\n",
    "# Pivot table do salario bruto - analise LOCAL\n",
    "pivot_comisson4 = planilha_geral.pivot_table(\n",
    "    columns='Arq_origem', values='Salario Bruto', aggfunc='sum', fill_value=0\n",
    ").reset_index().rename(columns={'index': 'Socio'})\n",
    "\n",
    "# Create a new column \"Total\" summing all numeric columns\n",
    "pivot_comisson4['Total'] = pivot_comisson4.select_dtypes(include='number').sum(axis=1)\n",
    "\n",
    "# Concatenate all three DataFrames vertically\n",
    "relatorio_comissao_local = pd.concat([relatorio_comissao_local, pivot_comisson4], ignore_index=True)\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = relatorio_comissao_local.select_dtypes(include='number').columns\n",
    "\n",
    "# Compute the difference between the last row and the second-to-last row\n",
    "diferenca_values = relatorio_comissao_local.iloc[-1, :][numeric_cols] - relatorio_comissao_local.iloc[-2, :][numeric_cols]\n",
    "\n",
    "# Create the \"Diferença\" row\n",
    "diferenca_row = pd.DataFrame([['Diferença'] + diferenca_values.tolist()], columns=['Socio'] + list(numeric_cols))\n",
    "\n",
    "# Append the \"Diferença\" row to the DataFrame\n",
    "relatorio_comissao_local = pd.concat([relatorio_comissao_local, diferenca_row], ignore_index=True)\n",
    "\n",
    "save_df_to_excel(relatorio_comissao_local, f\"{data_arquivo}_relatorio_comissao_local.xlsx\", nome_aba='Sheet1')\n",
    "######## ---------------------------- RELATORIO 2  -  Fim  --------------------------------- ###########\n",
    "\n",
    "\n",
    "\n",
    "######## ---------------------------- RELATORIO 3 - POR SOCIO - Começo --------------------------------- ###########\n",
    "########################################### Analise Rest ###########################################\n",
    "# Filter rows where 'Socio_2' is 'Vinicio'\n",
    "vinicio_socio = planilha_geral[planilha_geral['Socio_2'] == 'Vinicio']\n",
    "\n",
    "# Create pivot table for 'Vinicio'\n",
    "pivot_socio_vinicio = vinicio_socio.pivot_table(\n",
    "    index='Socio_3', \n",
    "    values=['Salario Bruto', 'Comission_1', 'Comission_2', 'Comission_3'], \n",
    "    aggfunc='sum', \n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns to Vinicio Analysis\n",
    "pivot_socio_vinicio = pivot_socio_vinicio.rename(columns={\n",
    "    'Socio_3': 'Comissao Analises',\n",
    "    'Comission_1': 'Guilherme',\n",
    "    'Comission_2': 'Vinicio',\n",
    "    'Comission_3': 'Socio'\n",
    "})\n",
    "\n",
    "# Filter rows where 'Socio_2' is NOT 'Vinicio'\n",
    "outros_socio2 = planilha_geral[planilha_geral['Socio_2'] != 'Vinicio']\n",
    "\n",
    "# List to store pivot tables\n",
    "pivot_list = [pivot_socio_vinicio]  # Start with Vinicio's pivot table\n",
    "\n",
    "# Ensure there are rows to process\n",
    "if not outros_socio2.empty:\n",
    "    # Iterate through unique 'Socio_2' Analysis\n",
    "    for socio in outros_socio2['Socio_2'].unique():\n",
    "        # Create a new DataFrame for each 'Socio_2' value\n",
    "        df_socio = outros_socio2[outros_socio2['Socio_2'] == socio].copy()\n",
    "        \n",
    "        # Create pivot table\n",
    "        pivot_nova2 = df_socio.pivot_table(\n",
    "            index='Socio_3', \n",
    "            values=['Salario Bruto', 'Comission_1', 'Comission_2', 'Comission_3'], \n",
    "            aggfunc='sum', \n",
    "            fill_value=0\n",
    "        ).reset_index()\n",
    "\n",
    "        # Rename columns\n",
    "        pivot_nova2 = pivot_nova2.rename(columns={\n",
    "            'Socio_3': 'Comissao Analises',\n",
    "            'Comission_1': 'Guilherme',\n",
    "            'Comission_2': f'{socio}',  # Use unique Socio_2 name as column header\n",
    "            'Comission_3': 'Socio'\n",
    "        })\n",
    "\n",
    "        # Append to the list\n",
    "        pivot_list.append(pivot_nova2)\n",
    "\n",
    "# Concatenate all pivot tables in the list\n",
    "relatorio_comissao_socios_temp = pd.concat(pivot_list, ignore_index=True)\n",
    "\n",
    "# Replace '-' with 'Guilherme' in 'Comissao Analises'\n",
    "relatorio_comissao_socios_temp['Comissao Analises'] = relatorio_comissao_socios_temp['Comissao Analises'].replace('-', 'Guilherme')\n",
    "\n",
    "# Grouping by 'Comissao Analises' and sorting alphabetically\n",
    "relatorio_comissao_socios = relatorio_comissao_socios_temp.groupby('Comissao Analises').sum().reset_index().sort_values(by='Comissao Analises')\n",
    "\n",
    "# Check if 'NOVO PESSOAL' exists in the 'Comissao Analises' column\n",
    "if 'NOVO PESSOAL' in relatorio_comissao_socios['Comissao Analises'].values:\n",
    "    # Move 'NOVO PESSOAL' to the top\n",
    "    relatorio_comissao_socios = pd.concat([\n",
    "        relatorio_comissao_socios[relatorio_comissao_socios['Comissao Analises'] == 'NOVO PESSOAL'],\n",
    "        relatorio_comissao_socios[relatorio_comissao_socios['Comissao Analises'] != 'NOVO PESSOAL']\n",
    "    ], ignore_index=True)\n",
    "\n",
    "\n",
    "# Move 'Guilherme' to the top\n",
    "relatorio_comissao_socios = pd.concat([\n",
    "    relatorio_comissao_socios[relatorio_comissao_socios['Comissao Analises'] == 'Guilherme'],\n",
    "    relatorio_comissao_socios[relatorio_comissao_socios['Comissao Analises'] != 'Guilherme']\n",
    "])\n",
    "\n",
    "# Check if column '-' exists and drop it\n",
    "if '-' in relatorio_comissao_socios.columns:\n",
    "    relatorio_comissao_socios = relatorio_comissao_socios.drop(columns=['-'])\n",
    "\n",
    "# Move 'Salario Bruto' to index 1\n",
    "cols = list(relatorio_comissao_socios.columns)\n",
    "cols.remove('Salario Bruto')  # Remove from the original position\n",
    "cols.insert(1, 'Salario Bruto')  # Insert at index 1\n",
    "relatorio_comissao_socios = relatorio_comissao_socios[cols]\n",
    "\n",
    "# Ensure there are rows to process\n",
    "if not outros_socio2.empty:\n",
    "    # Selecionar todas as colunas numéricas, exceto 'Salario Bruto'\n",
    "    numeric_cols = [col for col in relatorio_comissao_socios.select_dtypes(include='number').columns if col != 'Salario Bruto']\n",
    "\n",
    "    # Calculate 'Total Comissao' summing all numeric columns except 'Salario Bruto'\n",
    "    relatorio_comissao_socios[\"Total Comissao\"] = relatorio_comissao_socios[numeric_cols].sum(axis=1)\n",
    "\n",
    "    # Define fixed column order\n",
    "    first_cols = [\"Comissao Analises\", \"Salario Bruto\", \"Total Comissao\", \"Guilherme\", \"Vinicio\", \"Socio\"]\n",
    "\n",
    "    # Identify all other columns dynamically (excluding the first ones)\n",
    "    other_cols = [col for col in relatorio_comissao_socios.columns if col not in first_cols]\n",
    "\n",
    "    # Define the final column order\n",
    "    final_order = first_cols + other_cols\n",
    "\n",
    "    # Reorder DataFrame\n",
    "    relatorio_comissao_socios = relatorio_comissao_socios[final_order]\n",
    "\n",
    "\n",
    "# CASO SO EXISTA GUILHERME E VINICIO NA SEMANA\n",
    "else:\n",
    "    # Add 'Total Comissao' at index 2 (summing specified columns)\n",
    "    relatorio_comissao_socios.insert(2, 'Total Comissao', \n",
    "                                     relatorio_comissao_socios[['Guilherme', 'Vinicio', 'Socio']].sum(axis=1))\n",
    "\n",
    "\n",
    "# Calcular a soma das colunas numéricas\n",
    "total_row = relatorio_comissao_socios.select_dtypes(include='number').sum().to_frame().T\n",
    "\n",
    "# Adicionar um rótulo na coluna 'Comissao Analises' para identificar a linha de totais\n",
    "total_row.insert(0, 'Comissao Analises', 'Total')\n",
    "\n",
    "# Garantir que as colunas estejam alinhadas antes de concatenar\n",
    "total_row = total_row.reindex(columns=relatorio_comissao_socios.columns, fill_value='')\n",
    "\n",
    "# Anexar a linha de totais ao DataFrame\n",
    "relatorio_comissao_socios = pd.concat([relatorio_comissao_socios, total_row], ignore_index=True)\n",
    "\n",
    "# Fill NaN values with 0\n",
    "relatorio_comissao_socios = relatorio_comissao_socios.fillna(0)\n",
    "\n",
    "save_df_to_excel(relatorio_comissao_socios, f\"{data_arquivo}_relatorio_comissao_socios.xlsx\", nome_aba='Sheet1')\n",
    "######## ---------------------------- RELATORIO 3  -  Fim  --------------------------------- ###########\n",
    "\n",
    "\n",
    "print('\\n- Planilha geral de dados e relatorio criados com sucesso.\\n')\n",
    "\n",
    "# Lista de colunas a serem analisadas\n",
    "analises_columns = ['UUID do motorista', 'Origem_Ref', 'NOME', 'IBAN', 'Arq_origem', 'Salario Bruto', 'Comissao_Geral', 'DESCONTO ALUGUEL SEMANA', 'Deve(+)','DESC. TRANSF BANCARIA', 'Valor a Pagar']\n",
    "\n",
    "# Seleciona as colunas relevantes do DataFrame original\n",
    "arquivo_analises = planilha_geral[analises_columns]\n",
    "\n",
    "# Reinicia o índice do DataFrame para garantir que seja uma sequência numérica\n",
    "arquivo_analises.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Agrupa os dados pela coluna 'Origem_Ref' e calcula a soma das outras colunas\n",
    "grouped_analises = arquivo_analises.groupby('Origem_Ref').sum(numeric_only=True)\n",
    "\n",
    "# Reinicia o índice após o agrupamento para converter 'Origem_Ref' de índice para uma coluna\n",
    "grouped_analises.reset_index(inplace=True)\n",
    "\n",
    "# Salva o DataFrame em um arquivo Excel\n",
    "#save_df_to_excel(grouped_analises, f\"{data_arquivo}_Resumo por app.xlsx\", 'Resumo por app')\n",
    "\n",
    "# Retorna o DataFrame agrupado\n",
    "grouped_analises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d8e71",
   "metadata": {},
   "source": [
    "# Arquivo de drivers para pagamento\n",
    "<h6>celula_5</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be72ac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/3ylp0mwj1c37m5vjr6slv2qc0000gn/T/ipykernel_19816/1382232230.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  arquivo_socio['first'] = arquivo_socio['first'].str.slice(0, 16)\n",
      "/var/folders/vr/3ylp0mwj1c37m5vjr6slv2qc0000gn/T/ipykernel_19816/1382232230.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  arquivo_socio['last'] = arquivo_socio['last'].str.slice(0, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Verifique arquivo \"sem_iban\" como pessoas que nao entrarão no arquivo para pagamento de pagamento. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/xlsxwriter/workbook.py:369: UserWarning: Calling close() on already closed file.\n",
      "  warn(\"Calling close() on already closed file.\")\n"
     ]
    }
   ],
   "source": [
    "########################### PRIMEIRA ANALISE - LUCRO INDIVIDUAL DOS PARCEIROS ###########################\n",
    "# Separar colunas para arquivo individual dos socios\n",
    "arquivo_socio_columns = ['UUID do motorista', 'first', 'last', 'Salario Bruto', 'Arq_origem']\n",
    "arquivo_socio = planilha_geral[arquivo_socio_columns]\n",
    "\n",
    "# Truncar as colunas 'first' e 'last' para no máximo 16 caracteres\n",
    "arquivo_socio['first'] = arquivo_socio['first'].str.slice(0, 16)\n",
    "arquivo_socio['last'] = arquivo_socio['last'].str.slice(0, 16)\n",
    "\n",
    "\n",
    "# Criando o arquivo para a coluna comission 2 com os valores based da coluna comissao 2\n",
    "table_1_temp = pd.pivot_table(planilha_geral, values='Comission_1', index=['Socio_1'], \n",
    "                           columns=['UUID do motorista'], fill_value=0, aggfunc='sum').reset_index()\n",
    "\n",
    "# Filter rows where 'Socio_1' is '-'\n",
    "table_1 = table_1_temp[table_1_temp['Socio_1'] == '-'].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "table_1 = table_1.rename(columns={'Socio_1': 'Socio'})\n",
    "table_1['Socio'] = 'New'\n",
    "\n",
    "# Criando o arquivo para a coluna comission 2 com os valores based da coluna comissao 2\n",
    "table_2 = pd.pivot_table(planilha_geral, values='Comission_2', index=['Socio_2'], \n",
    "                           columns=['UUID do motorista'], fill_value=0, aggfunc='sum').reset_index()\n",
    "\n",
    "# Renomeando a coluna para o concat funcionar\n",
    "table_2 = table_2.rename(columns={'Socio_2': 'Socio'})\n",
    "\n",
    "# Criando o arquivo para a coluna comission 3 com os valores based da coluna comissao 3\n",
    "table_3 = pd.pivot_table(planilha_geral, values='Comission_3', index=['Socio_3'], \n",
    "                           columns=['UUID do motorista'], fill_value=0, aggfunc='sum').reset_index()\n",
    "\n",
    "# Renomeando a coluna para o concat funcionar\n",
    "table_3 = table_3.rename(columns={'Socio_3': 'Socio'})\n",
    "\n",
    "# Juntando os arquivos dos socios em uma tabela\n",
    "merged_socios = pd.concat([table_1, table_2, table_3], ignore_index=True).fillna(0)\n",
    "\n",
    "# Agrupando parceiros em um resumo com nomes unicos\n",
    "merged_socios = merged_socios.groupby('Socio').agg('sum').reset_index()\n",
    "merged_socios_transposed = merged_socios.transpose().reset_index()\n",
    "\n",
    "# Arrumando o cabeçalho e reestruturando os arquivos.\n",
    "merged_socios_transposed.columns = merged_socios_transposed.iloc[0]  # Set the first row as column names\n",
    "merged_socios_transposed = merged_socios_transposed[1:]  # Remove the first row\n",
    "merged_socios_transposed = merged_socios_transposed.rename(columns={'Socio': 'UUID do motorista'})\n",
    "\n",
    "\n",
    "# Função para salvar DataFrame como PDF com várias páginas\n",
    "def save_to_pdf(df, filename):\n",
    "    pdf = SimpleDocTemplate(filename, pagesize=A4)\n",
    "    elements = []\n",
    "    \n",
    "    # Convert the DataFrame to a list of lists\n",
    "    data = [df.columns.tolist()] + df.values.tolist()\n",
    "    \n",
    "    # Create a table with the content of the DataFrame\n",
    "    table = Table(data)\n",
    "    \n",
    "    # Calculate column widths to fit within the A4 page width\n",
    "    column_widths = [pdf.width / len(df.columns) for _ in df.columns]\n",
    "    table._argW = column_widths\n",
    "    \n",
    "    # Set the table style\n",
    "    table.setStyle(TableStyle([\n",
    "        ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n",
    "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "        ('FONTSIZE', (0, 0), (-1, -1), 8),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n",
    "        ('GRID', (0, 0), (-1, -1), 0.5, colors.black),\n",
    "    ]))\n",
    "    \n",
    "    elements.append(table)\n",
    "    pdf.build(elements)\n",
    "    \n",
    "    \n",
    "# Lendo cada coluna e procurando somente por colunas com valores acima de 0\n",
    "for column in merged_socios_transposed.columns[1:]:\n",
    "    \n",
    "    # Filtrando linhas onde o valor da coluna é maior que 0\n",
    "    filtered_df = merged_socios_transposed[merged_socios_transposed[column] > 0]\n",
    "\n",
    "    # Cria um nome de arquivo para cada coluna\n",
    "    filename_temp = f'{column}_individual_{data}.pdf'\n",
    "    filename = os.path.join(new_folder_path, filename_temp)\n",
    "    \n",
    "    # Salva 'UUID do motorista' e a coluna atual em PDF, apenas linhas onde coluna > 0\n",
    "    filtered_df = filtered_df[['UUID do motorista', column]]\n",
    "    \n",
    "    # Junta arquivos com as colunas escolhidas da planilha de dados\n",
    "    arquivo_separado = pd.merge(filtered_df, arquivo_socio, on='UUID do motorista', how='left')\n",
    "    \n",
    "    # Define the correct column order as a list\n",
    "    order_recibo_col = ['Arq_origem', 'first', 'last', 'Salario Bruto', column]  # Fixing the structure\n",
    "\n",
    "    # Reorder the DataFrame columns\n",
    "    arquivo_separado = arquivo_separado[order_recibo_col]\n",
    "\n",
    "    # Calculate total sum for 'Salario Bruto' and column\n",
    "    total_salario = round(arquivo_separado['Salario Bruto'].sum(), 2)\n",
    "    total_col = round(arquivo_separado[column].sum(), 2)\n",
    "\n",
    "    # Create the total row with required values\n",
    "    total_row = pd.DataFrame({\n",
    "        'Arq_origem': ['Total'],  # Set 'Total' in 'Arq_origem'\n",
    "        'first': ['-'],           # Set '-' in 'first'\n",
    "        'last': ['-'],            # Set '-' in 'last'\n",
    "        'Salario Bruto': [total_salario],  # Sum for 'Salario Bruto'\n",
    "        column: [total_col]       # Sum for the specific column\n",
    "    })\n",
    "\n",
    "    # Append the total row to the DataFrame\n",
    "    arquivo_separado = pd.concat([arquivo_separado, total_row], ignore_index=True)\n",
    "\n",
    "    # Checa se o total eh a cima de 0\n",
    "    if arquivo_separado[column].sum() > 0 and column != 'New':\n",
    "        # Salva o arquivo em PDF\n",
    "        save_to_pdf(arquivo_separado, filename)\n",
    "        \n",
    "    #else:\n",
    "        #agora_vai = pd.DataFrame(arquivo_separado)\n",
    "        #save_df_to_excel(agora_vai, f\"{data_arquivo}_PESSOAL NOVO.xlsx\", nome_aba='Sheet1')\n",
    "       \n",
    "\n",
    "\n",
    "###########################  ZIPPAR E LIMPAR FOLDER  ###########################\n",
    "# Define the directory where the PDFs are located\n",
    "#pdf_directory = os.getcwd()  # Change this if needed\n",
    "# Find all PDF files in the directory\n",
    "pdf_files = glob.glob(os.path.join(new_folder_path, \"*.pdf\"))\n",
    "\n",
    "# Define the name of the ZIP file\n",
    "zip_filename_temp = \"RECIBOS SOCIOS.zip\"\n",
    "zip_filename = os.path.join(new_folder_path, zip_filename_temp)\n",
    "\n",
    "# Create a ZIP file and add all PDF files\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for pdf in pdf_files:\n",
    "        zipf.write(pdf, os.path.basename(pdf))  # Add file to ZIP with its base name\n",
    "\n",
    "# Delete all original PDF files\n",
    "for pdf in pdf_files:\n",
    "    os.remove(pdf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################### SEGUNDA ANALISE - ARQUIVO PARA IR AO BANCO ###########################\n",
    "# Criando a analise das colunas de comissão\n",
    "pivot_table_1 = planilha_geral.groupby('Socio_1').sum(numeric_only=True).reset_index()\n",
    "\n",
    "# Calculate 'Comissao_Final' as the sum of individual columns\n",
    "pivot_table_1['Comissao_Final'] = (pivot_table_1['Comission_1'] + \n",
    "                                   pivot_table_1['DESCONTO ALUGUEL SEMANA'] +    \n",
    "                                   pivot_table_1['DESC. TRANSF BANCARIA'] - \n",
    "                                   pivot_table_1['Deve(+)']).round(2)\n",
    "\n",
    "# Separando a primeira e a ultima coluna com o valor resumo do socio_1 - Guilherme\n",
    "pivot_table_1 = pivot_table_1.iloc[:, [0, -1]]\n",
    "\n",
    "# Renomeando colunas\n",
    "pivot_table_1.rename(columns={pivot_table_1.columns[0]: 'NOME', pivot_table_1.columns[-1]: 'Valor a Pagar'}, inplace=True)\n",
    "\n",
    "# Separando a coluna do socio_2 e fazer a divisao do lucro\n",
    "pivot_table_2 = planilha_geral.groupby('Socio_2')['Comission_2'].sum(numeric_only=True).reset_index()\n",
    "pivot_table_2.rename(columns={pivot_table_2.columns[0]: 'NOME', pivot_table_2.columns[1]: 'Valor a Pagar'}, inplace=True)\n",
    "\n",
    "# Separando a coluna do socio_3 e fazer a divisao do lucro\n",
    "pivot_table_3 = planilha_geral.groupby('Socio_3')['Comission_3'].sum(numeric_only=True).reset_index()\n",
    "pivot_table_3.rename(columns={pivot_table_3.columns[0]: 'NOME', pivot_table_3.columns[1]: 'Valor a Pagar'}, inplace=True)\n",
    "\n",
    "# Juntando as tres coluns em um so arquivo\n",
    "merged_pivot = pd.concat([pivot_table_1, pivot_table_2, pivot_table_3], ignore_index=True).fillna(0)\n",
    "\n",
    "# Agrupando parceiros em um resumo com nomes unicos\n",
    "merged_pivot = merged_pivot.groupby('NOME').agg({'Valor a Pagar': 'sum' }).reset_index()\n",
    "\n",
    "# Excluindo zeros da analises\n",
    "merged_pivot = merged_pivot[merged_pivot['Valor a Pagar'].astype(float) > 0]\n",
    "\n",
    "# Separando as contas dos socios para criacao do arquivo ao banco\n",
    "outras_info_subset = outras_info[['NOME', 'IBAN', 'Check.IBAN.Socio']]\n",
    "\n",
    "# Organizando os dados e renomeando as colunas da panilha - uber\n",
    "arquivo_comissao_pagamento = pd.merge(merged_pivot, outras_info_subset, on=\"NOME\", how=\"left\").fillna(0)\n",
    "\n",
    "# Renomeando coluna da validacao do IBAN\n",
    "arquivo_comissao_pagamento = arquivo_comissao_pagamento.rename(columns={'Check.IBAN.Socio': 'Check.IBAN'})\n",
    "\n",
    "# Convert 'NOME' column to string and trim spaces\n",
    "arquivo_comissao_pagamento['NOME'] = arquivo_comissao_pagamento['NOME'].astype(str)\n",
    "\n",
    "# Adiciona a referencia como Parceiros Comissao\n",
    "arquivo_comissao_pagamento['Origem_Ref'] = 'Parceiros_Comissao'\n",
    "\n",
    "# Crie um arquivo retirado da planilha geral com colunas abaixo - valores a serem pagos\n",
    "pagamento_columns = ['UUID do motorista', 'first', 'last', 'NOME', 'IBAN', 'Arq_origem', 'Check.IBAN', 'Valor a Pagar', 'Valor Comissão', 'Salario Bruto','DESC. TRANSF BANCARIA','Origem_Ref']\n",
    "arquivo_driver_pagamento = planilha_geral[pagamento_columns]\n",
    "\n",
    "# Juntando os dois arquivos para pagamento \n",
    "arquivo_driver_parceiro = pd.concat([arquivo_comissao_pagamento, arquivo_driver_pagamento], ignore_index=True).fillna(0)\n",
    "\n",
    "# Verificando quem nao possui IBAN e criando arquivo \"sem_iban\" se necessario.\n",
    "sem_iban = arquivo_driver_parceiro[~arquivo_driver_parceiro['Check.IBAN'].astype(str).str.contains('OK')]\n",
    "\n",
    "####################################  ANALISE SEM IBAN ####################################\n",
    "# Verificacao com if se sem iban existe\n",
    "if len(sem_iban) > 0:\n",
    "    # Remove os Parceiros do relatorio final\n",
    "    sem_iban = sem_iban[sem_iban['Origem_Ref'] != 'Parceiros_Comissao']\n",
    "\n",
    "    # Concatenating 'first' and 'last' into 'Novo Nome'\n",
    "    sem_iban['Novo Nome'] = sem_iban['first'] + ' ' + sem_iban['last']\n",
    "\n",
    "    # Organizando o relatorio final\n",
    "    columns_list_sem_iban = [\n",
    "        \"Origem_Ref\", \"Arq_origem\", \"NOME\", \"Novo Nome\", \"IBAN\", \n",
    "        \"UUID do motorista\", \"Salario Bruto\", \"Valor a Pagar\", 'Valor Comissão']\n",
    "    sem_iban = sem_iban[columns_list_sem_iban]\n",
    "\n",
    "\n",
    "    sem_iban = sem_iban.sort_values(by='Arq_origem').reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Define numeric columns to sum\n",
    "    columns_to_sum = [\"Valor a Pagar\", \"Salario Bruto\", 'Valor Comissão']\n",
    "\n",
    "    # Calculate total sum for the specified columns\n",
    "    total_values = sem_iban[columns_to_sum].sum().round(2)\n",
    "\n",
    "    # Create a total row with column names and corresponding sums\n",
    "    total_row = pd.DataFrame({\n",
    "        col: [\"Total\" if i == 0 else total_values.get(col, \"-\")] if col in columns_to_sum or i == 0 else [\"-\"]\n",
    "        for i, col in enumerate(sem_iban.columns)})\n",
    "\n",
    "    # Append the total row at the bottom of the DataFrame\n",
    "    sem_iban = pd.concat([sem_iban, total_row], ignore_index=True)\n",
    "\n",
    "    #################################### Adicionando EXTRA INFORMAÇÕES ####################################\n",
    "    sem_iban['Socio_1'] = 'Guilherme'\n",
    "    sem_iban['Socio_2'] = ''\n",
    "    sem_iban['Socio_3'] = ''\n",
    "\n",
    "    # Get the total number of rows\n",
    "    num_rows = len(sem_iban)\n",
    "\n",
    "    # Generate the Excel formula dynamically for each row em comissao_1\n",
    "    sem_iban['comissao_1'] = [\n",
    "        f'=IF(AND(K{i}=\"\", L{i}=\"\"), I{i}, '\n",
    "        f'IF(AND(K{i}<>\"\", L{i}=\"\"), I{i}/2, '\n",
    "        f'IF(AND(L{i}<>\"\", K{i}=\"\"), I{i}/2, '\n",
    "        f'IF(AND(K{i}<>\"\", L{i}<>\"\"), I{i}*0.30, \"\"))))'\n",
    "        for i in range(2, num_rows + 2)  # Adjusting row numbers dynamically\n",
    "    ]\n",
    "\n",
    "    # Generate formula for comissao_2 correctly\n",
    "    sem_iban['comissao_2'] = [\n",
    "        f'=IF(K{i}=\"\", 0, '\n",
    "        f'IF(AND(K{i}<>\"\", L{i}=\"\"), I{i}/2, '\n",
    "        f'IF(AND(K{i}<>\"\", L{i}<>\"\"), I{i}*0.30, \"\")))'\n",
    "        for i in range(2, num_rows + 2)  # Adjusting row numbers dynamically\n",
    "    ]\n",
    "\n",
    "    # Generate the Excel formula dynamically for each row in 'comissao_3'\n",
    "    sem_iban['comissao_3'] = [\n",
    "        f'=IF(L{i}=\"\", 0, '\n",
    "        f'IF(AND(L{i}<>\"\", K{i}=\"\"), I{i}/2, '\n",
    "        f'IF(AND(K{i}<>\"\", L{i}<>\"\"), I{i}*0.40, \"\")))'\n",
    "        for i in range(2, num_rows + 2)  # Adjusting row numbers dynamically\n",
    "    ]\n",
    "\n",
    "    # Set the last row in Socio_1 to \"-\" - DEVIDO a UNIQUE FORMULA\n",
    "    sem_iban.loc[num_rows - 1, 'Socio_1'] = \"\"\n",
    "\n",
    "    # Define the columns and their corresponding Excel column letters\n",
    "    columns_to_sum = {'comissao_1': 'M', 'comissao_2': 'N', 'comissao_3': 'O'}\n",
    "\n",
    "    # Insert the SUM formula dynamically for each column\n",
    "    for col_name, excel_col in columns_to_sum.items():\n",
    "        sum_formula = f\"=SUM({excel_col}2:{excel_col}{num_rows})\"  # Exclude last row from sum\n",
    "        sem_iban.loc[num_rows - 1, col_name] = sum_formula\n",
    "\n",
    "\n",
    "    sem_iban_name_file = f\"{data_arquivo}_sem_iban.xlsx\"\n",
    "\n",
    "    # Excuta a função previaviamente criada\n",
    "    save_df_to_excel(sem_iban, sem_iban_name_file, 'sem_iban')\n",
    "    \n",
    "    # Create a folder using the current date\n",
    "    folder_with_date = os.path.join(new_folder_path, sem_iban_name_file)\n",
    "    re_updating_sem_iban = pd.read_excel(folder_with_date)\n",
    "\n",
    "    # Save again with specific column widths\n",
    "    output_file_temp = f\"{data_arquivo}_sem_iban.xlsx\"\n",
    "    output_file = os.path.join(new_folder_path, output_file_temp)\n",
    "    \n",
    "\n",
    "    with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n",
    "        \n",
    "        sem_iban.to_excel(writer, sheet_name='sem_iban', index=False)\n",
    "\n",
    "        # Get the workbook and worksheet\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets['sem_iban']\n",
    "\n",
    "    \n",
    "        # Auto-adjust other columns\n",
    "        for i, col in enumerate(sem_iban.columns):\n",
    "            if col not in [\"comissao_1\", \"comissao_2\", \"comissao_3\", \"NOME\"]:\n",
    "                max_length = max(\n",
    "                    sem_iban[col].astype(str).map(len).max() if not sem_iban[col].isna().all() else 0,\n",
    "                    len(col)\n",
    "                ) + 5  # Add padding\n",
    "                worksheet.set_column(i, i, max_length)\n",
    "\n",
    "        #### ANALISE 1 ----------\n",
    "        # Add the UNIQUE formula in column R, row 1\n",
    "        worksheet.write_formula(\"R1\", '=FILTER(UNIQUE(J:J), UNIQUE(J:J)<>0)')\n",
    "\n",
    "        # Add \"soma_1\" in column S, row 1\n",
    "        worksheet.write(\"S1\", \"soma_1\")\n",
    "\n",
    "        # Insert the IF + SUMIFS formula in column S (rows 2 to 30)\n",
    "        for i in range(2, 31):  # Rows 2 to 30 (Excel uses 1-based indexing)\n",
    "            worksheet.write_formula(f\"S{i}\", \n",
    "                f'=IF(OR(R{i}<>0, R{i}<>\"\"), SUMIFS(M:M, J:J, R{i}), \"\")')\n",
    "\n",
    "\n",
    "        #### ANALISE 2 ------------\n",
    "        # Add the UNIQUE formula in column U, row 1\n",
    "        worksheet.write_formula(\"U1\", \"=FILTER(UNIQUE(K:K), UNIQUE(K:K)<>0)\")\n",
    "\n",
    "        # Add \"soma_2\" in column V, row 1\n",
    "        worksheet.write(\"V1\", \"soma_2\")\n",
    "\n",
    "        # Insert the IF + SUMIFS formula in column U (rows 2 to 30)\n",
    "        for i in range(2, 31):  # Rows 2 to 30\n",
    "            worksheet.write_formula(f\"V{i}\", \n",
    "                f'=IF(OR(U{i}<>0, U{i}<>\"\"), SUMIFS(N:N, K:K, U{i}), \"\")')\n",
    "\n",
    "\n",
    "        #### ANALISE 3 -----------\n",
    "        # Add the UNIQUE formula in column X, row 1\n",
    "        worksheet.write_formula(\"X1\", \"=FILTER(UNIQUE(L:L), UNIQUE(L:L)<>0)\")\n",
    "\n",
    "        # Add \"soma_2\" in column Y, row 1\n",
    "        worksheet.write(\"Y1\", \"soma_3\")\n",
    "\n",
    "        # Insert the IF + SUMIFS formula in column Y (rows 2 to 30)\n",
    "        for i in range(2, 31):  # Rows 2 to 30\n",
    "            worksheet.write_formula(f\"Y{i}\", \n",
    "                f'=IF(OR(X{i}<>0, X{i}<>\"\"), SUMIFS(O:O, L:L, X{i}), \"\")')\n",
    "\n",
    "        # Define number format for columns M to Y\n",
    "        number_format = workbook.add_format({'num_format': '#,##0.00'})\n",
    "\n",
    "        # Apply the number format to columns M to Y\n",
    "        worksheet.set_column('M:Y', None, number_format)  # None keeps the default width\n",
    "\n",
    "        # Define column widths manually\n",
    "        worksheet.set_column(\"C:C\", 20)  # Adjust if 'NOME' is in a different column\n",
    "        worksheet.set_column(\"M:M\", 13)  # Adjust this if 'comissao_1' is in a different column\n",
    "        worksheet.set_column(\"N:N\", 13)  # Adjust if 'comissao_2' is in a different column\n",
    "        worksheet.set_column(\"O:O\", 13)  # Adjust if 'comissao_3' is in a different column\n",
    "       \n",
    "        # Save the updated Excel file\n",
    "        writer.close()\n",
    "        \n",
    "    print('\\n\\n Verifique arquivo \"sem_iban\" como pessoas que nao entrarão no arquivo para pagamento de pagamento. \\n')\n",
    "\n",
    "########################### QUEBRANDO O ARQUIVO EM DIFERENTE ARQUIVOS PARA RODAR MAIS DE UMA AUTOMATIZACAO\n",
    "# Criando arquivo que ira para o banco\n",
    "vai_para_banco = arquivo_driver_parceiro[arquivo_driver_parceiro['Check.IBAN'].astype(str).str.contains('OK')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bfe039",
   "metadata": {},
   "source": [
    "<h6>celula_6</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0da7ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " *** Arquivo para automatizacao pronto ***\n",
      " *** Depois de tudo confirmado, avançe para rodar o XML *** \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total number of rows in the DataFrame\n",
    "total_rows = len(vai_para_banco)\n",
    "\n",
    "# Define how many parts you want to split your DataFrame into\n",
    "number_of_splits = 3\n",
    "\n",
    "# Calculate the number of rows for each file, and the remainder to add to the first few files\n",
    "base_size = total_rows // number_of_splits\n",
    "remainder = total_rows % number_of_splits\n",
    "\n",
    "# Create an array to hold the size of each split\n",
    "split_sizes = [base_size + 1 if i < remainder else base_size for i in range(number_of_splits)]\n",
    "\n",
    "# Initialize the starting index\n",
    "start_index = 0\n",
    "\n",
    "# Initialize an empty list to hold the DataFrame segments\n",
    "segments = []\n",
    "\n",
    "# Split the DataFrame and process each part\n",
    "for i, size in enumerate(split_sizes):\n",
    "    # Define the end index for this segment\n",
    "    end_index = start_index + size\n",
    "\n",
    "    # Extract the segment of the DataFrame and make a copy to avoid SettingWithCopyWarning\n",
    "    segment = vai_para_banco.iloc[start_index:end_index].copy()\n",
    "\n",
    "    # Create a column with the file name\n",
    "    segment['File_Name'] = f\"Banco{i + 1}\"\n",
    "\n",
    "    # Add the segment to the list\n",
    "    segments.append(segment)\n",
    "\n",
    "    # Update the start index for the next segment\n",
    "    start_index = end_index\n",
    "\n",
    "# Concatenate all segments into a single DataFrame\n",
    "banco_consolidado = pd.concat(segments, ignore_index=True)\n",
    "\n",
    "# Select only the visible columns\n",
    "visible_columns = [\"NOME\", \"Valor a Pagar\", \"IBAN\", \"File_Name\"]\n",
    "banco_consolidado_visible = banco_consolidado[visible_columns]\n",
    "\n",
    "# Define the file path for saving the Excel file\n",
    "banco_consolidado_visible_temp = f\"{data_arquivo}_Arquivo_Banco_consolidado.xlsx\"\n",
    "banco_consolidado_visible_path = os.path.join(new_folder_path, banco_consolidado_visible_temp)\n",
    "\n",
    "# Save the consolidated DataFrame to an Excel file\n",
    "banco_consolidado_visible.to_excel(banco_consolidado_visible_path, index=False)\n",
    "    \n",
    "print('\\n\\n\\n *** Arquivo para automatizacao pronto ***')\n",
    "print(' *** Depois de tudo confirmado, avançe para rodar o XML *** \\n\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29547ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d945df65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5f63b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML file generated: XMLBanco1.xml\n",
      "XML file generated: XMLBanco2.xml\n",
      "XML file generated: XMLBanco3.xml\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the file path for saving the Excel file\n",
    "banco_consolidado_visible_temp = f\"{data_arquivo}_Arquivo_Banco_consolidado.xlsx\"\n",
    "banco_consolidado_visible_path = os.path.join(new_folder_path, banco_consolidado_visible_temp)\n",
    "\n",
    "# Load data from Excel file\n",
    "payments_temp = pd.read_excel(banco_consolidado_visible_path)\n",
    "\n",
    "# Select the relevant columns\n",
    "col_seguimentos_keep = [\"NOME\", \"Valor a Pagar\", \"IBAN\", \"File_Name\"]\n",
    "payments_region = payments_temp[col_seguimentos_keep]\n",
    "\n",
    "# Loop through each unique region (based on 'File_Name')\n",
    "for file_name_col in payments_region['File_Name'].unique():\n",
    "    # Filter the DataFrame for the current region and make a copy\n",
    "    payments = payments_region[payments_region['File_Name'] == file_name_col].copy()\n",
    "\n",
    "    # Rename columns for consistency\n",
    "    payments.columns = [\"Creditor\", \"Amount\", \"IBAN\", \"Payment_ID\"]\n",
    "\n",
    "    # Convert all values in the 'Creditor' column to uppercase and limit the length to 28 characters\n",
    "    payments[\"Creditor\"] = payments[\"Creditor\"].str.upper().str.slice(0, 28)\n",
    "\n",
    "    # Convert IBAN values to uppercase\n",
    "    payments[\"IBAN\"] = payments[\"IBAN\"].str.upper()\n",
    "\n",
    "    # Ensure 'Amount' is of type float\n",
    "    payments[\"Amount\"] = payments[\"Amount\"].astype(float)\n",
    "\n",
    "    # Assign the same 'Payment_ID' for all rows\n",
    "    payments[\"Payment_ID\"] = \"PAGAMENTO SEMANAL\"\n",
    "\n",
    "    # XML structure\n",
    "    root = ET.Element(\"Document\", {\n",
    "        \"xmlns\": \"urn:iso:std:iso:20022:tech:xsd:pain.001.001.03\",\n",
    "        \"xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\"\n",
    "    })\n",
    "    cstmr_cdt_trf_initn = ET.SubElement(root, \"CstmrCdtTrfInitn\")\n",
    "\n",
    "    # Group Header\n",
    "    grp_hdr = ET.SubElement(cstmr_cdt_trf_initn, \"GrpHdr\")\n",
    "\n",
    "    # Generate timestamp in format YYYYMMDDHHMM\n",
    "    timestamp = datetime.now().strftime('%Y%m%d%H%M')\n",
    "    # Dynamically assign it to your XML element\n",
    "    ET.SubElement(grp_hdr, \"MsgId\").text = f\"UPROFIT911B-{timestamp}\" # ATUALIZADO\n",
    "    #ET.SubElement(grp_hdr, \"MsgId\").text = \"UPROFIT911B-2025012111123062\"\n",
    "\n",
    "    ET.SubElement(grp_hdr, \"CreDtTm\").text = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    ET.SubElement(grp_hdr, \"NbOfTxs\").text = str(len(payments))\n",
    "    ET.SubElement(grp_hdr, \"CtrlSum\").text = f\"{payments['Amount'].sum():.2f}\"\n",
    "\n",
    "    initg_pty = ET.SubElement(grp_hdr, \"InitgPty\")\n",
    "    ET.SubElement(initg_pty, \"Nm\").text = \"3WAQUISECONDUZ UNIPESSOAL LDA\"\n",
    "\n",
    "    # Adding ID structure\n",
    "    initg_pty_id = ET.SubElement(initg_pty, \"Id\")\n",
    "    prvt_id = ET.SubElement(initg_pty_id, \"PrvtId\")\n",
    "    othr = ET.SubElement(prvt_id, \"Othr\")\n",
    "    ET.SubElement(othr, \"Id\").text = \"516574361\"\n",
    "\n",
    "    # Payment Information\n",
    "    pmt_inf = ET.SubElement(cstmr_cdt_trf_initn, \"PmtInf\")\n",
    "\n",
    "\n",
    "    ET.SubElement(pmt_inf, \"PmtInfId\").text = f\"UPROFIT911B-{timestamp}\"\n",
    "    #ET.SubElement(pmt_inf, \"PmtInfId\").text = \"UPROFIT911B-20250121111230\"\n",
    "\n",
    "    ET.SubElement(pmt_inf, \"PmtMtd\").text = \"TRF\"\n",
    "    ET.SubElement(pmt_inf, \"NbOfTxs\").text = str(len(payments))\n",
    "    ET.SubElement(pmt_inf, \"CtrlSum\").text = f\"{payments['Amount'].sum():.2f}\"\n",
    "\n",
    "    pmt_tp_inf = ET.SubElement(pmt_inf, \"PmtTpInf\")\n",
    "\n",
    "    # IMEDIATA TAG\n",
    "    lcl_instrm = ET.SubElement(pmt_tp_inf, \"LclInstrm\")\n",
    "    ET.SubElement(lcl_instrm, \"Prtry\").text = \"INST\"\n",
    "\n",
    "    ctgy_purp = ET.SubElement(pmt_tp_inf, \"CtgyPurp\")\n",
    "    ET.SubElement(ctgy_purp, \"Cd\").text = \"SALA\"\n",
    "\n",
    "\n",
    "    # Current date formatted as YYYY-MM-DD\n",
    "    current_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Dynamically assign it to your XML element\n",
    "    ET.SubElement(pmt_inf, \"ReqdExctnDt\").text = current_date\n",
    "\n",
    "    # Debtor Information\n",
    "    dbtr = ET.SubElement(pmt_inf, \"Dbtr\")\n",
    "    ET.SubElement(dbtr, \"Nm\").text = \"3WAQUISECONDUZ UNIPESSOAL LDA\"\n",
    "\n",
    "    dbtr_acct = ET.SubElement(pmt_inf, \"DbtrAcct\")\n",
    "    dbtr_acct_id = ET.SubElement(dbtr_acct, \"Id\")\n",
    "    ET.SubElement(dbtr_acct_id, \"IBAN\").text = \"PT50003300004565690070705\"\n",
    "\n",
    "    # Adding Debtor Agent information\n",
    "    dbtr_agt = ET.SubElement(pmt_inf, \"DbtrAgt\")\n",
    "    ET.SubElement(dbtr_agt, \"FinInstnId\").text = \"\"\n",
    "\n",
    "    # Loop through payments DataFrame\n",
    "    for row in payments.itertuples(index=False):\n",
    "        cdt_trf_tx_inf = ET.SubElement(pmt_inf, \"CdtTrfTxInf\")\n",
    "\n",
    "        pmt_id = ET.SubElement(cdt_trf_tx_inf, \"PmtId\")\n",
    "        ET.SubElement(pmt_id, \"EndToEndId\").text = row.Payment_ID if pd.notna(row.Payment_ID) else \"PAGAMENTO\"\n",
    "\n",
    "        amt = ET.SubElement(cdt_trf_tx_inf, \"Amt\")\n",
    "        ET.SubElement(amt, \"InstdAmt\", Ccy=\"EUR\").text = f\"{row.Amount:.2f}\"\n",
    "\n",
    "        cdtr_agt = ET.SubElement(cdt_trf_tx_inf, \"CdtrAgt\")\n",
    "        fin_instn_id = ET.SubElement(cdtr_agt, \"FinInstnId\")\n",
    "        fin_instn_id.text = \"\" \n",
    "\n",
    "        cdtr = ET.SubElement(cdt_trf_tx_inf, \"Cdtr\")\n",
    "        ET.SubElement(cdtr, \"Nm\").text = row.Creditor\n",
    "\n",
    "        cdtr_acct = ET.SubElement(cdt_trf_tx_inf, \"CdtrAcct\")\n",
    "        cdtr_acct_id = ET.SubElement(cdtr_acct, \"Id\")\n",
    "        ET.SubElement(cdtr_acct_id, \"IBAN\").text = row.IBAN\n",
    "\n",
    "    ## Converter data para yyyy-mm-dd\n",
    "    new_data = datetime.strptime(data_arquivo, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Converte o XML em string\n",
    "    xml_str = ET.tostring(root, encoding='utf-8')\n",
    "    parsed = minidom.parseString(xml_str)\n",
    "    pretty_xml = parsed.toprettyxml(indent=\"    \")  # indentação de 4 espaços\n",
    "\n",
    "    # Corrige a primeira linha (declaração XML)\n",
    "    pretty_xml = pretty_xml.replace('<?xml version=\"1.0\" ?>', '<?xml version=\"1.0\" encoding=\"utf-8\"?>')\n",
    "\n",
    "    # Divide o XML formatado em linhas\n",
    "    lines = pretty_xml.splitlines()\n",
    "\n",
    "    # Junta os atributos da tag <Document> em uma única linha\n",
    "    for i, line in enumerate(lines):\n",
    "        if '<Document' in line:\n",
    "            while not lines[i].strip().endswith('>'):\n",
    "                lines[i] = lines[i].strip() + ' ' + lines[i + 1].strip()\n",
    "                del lines[i + 1]\n",
    "            break\n",
    "\n",
    "    # Mantém a primeira linha (declaração XML) sem indentação\n",
    "    final_lines = [lines[0]]  # Primeira linha intacta\n",
    "\n",
    "    # Aplica 12 espaços nas demais linhas (exceto linhas vazias)\n",
    "    for line in lines[1:]:\n",
    "        final_lines.append(' ' * 12 + line if line.strip() else '')\n",
    "\n",
    "    # Recompõe o XML final\n",
    "    final_xml = '\\n'.join(final_lines)\n",
    "\n",
    "    # Nome do arquivo XML\n",
    "    nome_arquivo = f\"XML{file_name_col}.xml\"\n",
    "    caminho_arquivo = os.path.join(new_folder_path, nome_arquivo)\n",
    "\n",
    "    # Salva no arquivo\n",
    "    with open(caminho_arquivo, 'w', encoding='utf-8') as f:\n",
    "        f.write(final_xml)\n",
    "\n",
    "    print(f\"XML file generated: {nome_arquivo}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98d1a8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML file generated: BATCHSEMIBAN.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/3ylp0mwj1c37m5vjr6slv2qc0000gn/T/ipykernel_19816/3913374842.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  payments_sem_iban_validos[\"Amount\"] = payments_sem_iban_validos[\"Amount\"].astype(float)\n",
      "/var/folders/vr/3ylp0mwj1c37m5vjr6slv2qc0000gn/T/ipykernel_19816/3913374842.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  payments_sem_iban_validos[\"Payment_ID\"] = \"PAGAMENTO SEMANAL\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Nome do arquivo XML\n",
    "sem_iban_xml_temp = f\"{data_arquivo}_sem_iban.xlsx\"\n",
    "sem_iban_xml = os.path.join(new_folder_path, sem_iban_xml_temp)\n",
    "\n",
    "# Load data\n",
    "payments_sem_iban = pd.read_excel(sem_iban_xml)\n",
    "\n",
    "# Keep specific columns\n",
    "col_seguimentos_keep = [\"Novo Nome\", \"Valor a Pagar\", \"IBAN\", \"Arq_origem\"]\n",
    "payments_sem_iban = payments_sem_iban[col_seguimentos_keep]\n",
    "\n",
    "# Rename columns for consistency\n",
    "payments_sem_iban.columns = [\"Creditor\", \"Amount\", \"IBAN\", \"Payment_ID\"]\n",
    "\n",
    "# Convert 'Creditor' to uppercase and slice\n",
    "payments_sem_iban[\"Creditor\"] = payments_sem_iban[\"Creditor\"].str.upper().str.slice(0, 28)\n",
    "\n",
    "# Clean 'IBAN' column (keep only letters and numbers, remove special characters)\n",
    "payments_sem_iban[\"IBAN\"] = payments_sem_iban[\"IBAN\"].fillna('0')\n",
    "payments_sem_iban[\"IBAN\"] = payments_sem_iban[\"IBAN\"].astype(str).str.replace(r'[^A-Za-z0-9]', '', regex=True)\n",
    "payments_sem_iban[\"IBAN\"] = payments_sem_iban[\"IBAN\"].str.upper()\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to check IBAN rules\n",
    "def iban_check(iban):\n",
    "    if pd.isna(iban) or iban == '':\n",
    "        return False\n",
    "    first_char = iban[0]\n",
    "    if first_char.isalpha():\n",
    "        if first_char == 'P':\n",
    "            return len(iban) == 25\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Apply the function to filter correct IBANs\n",
    "payments_sem_iban_validos = payments_sem_iban[payments_sem_iban['IBAN'].apply(iban_check)]\n",
    "\n",
    "# Apply the function to filter invalid IBANs\n",
    "payments_sem_iban_nao_enviados = payments_sem_iban[~payments_sem_iban['IBAN'].apply(iban_check)]\n",
    "\n",
    "\n",
    "# Save invalid IBANs\n",
    "save_df_to_excel(payments_sem_iban_nao_enviados, f\"payments_sem_iban_nao_enviados.xlsx\", 'ao banco')\n",
    "\n",
    "# Continue with valid payments\n",
    "payments_sem_iban_validos[\"Amount\"] = payments_sem_iban_validos[\"Amount\"].astype(float)\n",
    "payments_sem_iban_validos[\"Payment_ID\"] = \"PAGAMENTO SEMANAL\"\n",
    "\n",
    "# Save valid payments\n",
    "save_df_to_excel(payments_sem_iban_validos, f\"payments_sem_iban.xlsx\", 'ao banco')\n",
    "\n",
    "\n",
    "payments_sem_iban = payments_sem_iban_validos\n",
    "\n",
    "# XML structure\n",
    "root = ET.Element(\"Document\", {\n",
    "    \"xmlns\": \"urn:iso:std:iso:20022:tech:xsd:pain.001.001.03\",\n",
    "    \"xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\"\n",
    "})\n",
    "cstmr_cdt_trf_initn = ET.SubElement(root, \"CstmrCdtTrfInitn\")\n",
    "\n",
    "# Group Header\n",
    "grp_hdr = ET.SubElement(cstmr_cdt_trf_initn, \"GrpHdr\")\n",
    "\n",
    "# Generate timestamp in format YYYYMMDDHHMM\n",
    "timestamp = datetime.now().strftime('%Y%m%d%H%M')\n",
    "# Dynamically assign it to your XML element\n",
    "ET.SubElement(grp_hdr, \"MsgId\").text = f\"UPROFIT911B-{timestamp}\" # ATUALIZADO\n",
    "#ET.SubElement(grp_hdr, \"MsgId\").text = \"UPROFIT911B-2025012111123062\"\n",
    "\n",
    "ET.SubElement(grp_hdr, \"CreDtTm\").text = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "ET.SubElement(grp_hdr, \"NbOfTxs\").text = str(len(payments_sem_iban))\n",
    "ET.SubElement(grp_hdr, \"CtrlSum\").text = f\"{payments_sem_iban['Amount'].sum():.2f}\"\n",
    "\n",
    "initg_pty = ET.SubElement(grp_hdr, \"InitgPty\")\n",
    "ET.SubElement(initg_pty, \"Nm\").text = \"3WAQUISECONDUZ UNIPESSOAL LDA\"\n",
    "\n",
    "# Adding ID structure\n",
    "initg_pty_id = ET.SubElement(initg_pty, \"Id\")\n",
    "prvt_id = ET.SubElement(initg_pty_id, \"PrvtId\")\n",
    "othr = ET.SubElement(prvt_id, \"Othr\")\n",
    "ET.SubElement(othr, \"Id\").text = \"516574361\"\n",
    "\n",
    "# Payment Information\n",
    "pmt_inf = ET.SubElement(cstmr_cdt_trf_initn, \"PmtInf\")\n",
    "\n",
    "\n",
    "ET.SubElement(pmt_inf, \"PmtInfId\").text = f\"UPROFIT911B-{timestamp}\"\n",
    "#ET.SubElement(pmt_inf, \"PmtInfId\").text = \"UPROFIT911B-20250121111230\"\n",
    "\n",
    "ET.SubElement(pmt_inf, \"PmtMtd\").text = \"TRF\"\n",
    "ET.SubElement(pmt_inf, \"NbOfTxs\").text = str(len(payments_sem_iban))\n",
    "ET.SubElement(pmt_inf, \"CtrlSum\").text = f\"{payments_sem_iban['Amount'].sum():.2f}\"\n",
    "\n",
    "pmt_tp_inf = ET.SubElement(pmt_inf, \"PmtTpInf\")\n",
    "\n",
    "# IMEDIATA TAG\n",
    "lcl_instrm = ET.SubElement(pmt_tp_inf, \"LclInstrm\")\n",
    "ET.SubElement(lcl_instrm, \"Prtry\").text = \"INST\"\n",
    "\n",
    "ctgy_purp = ET.SubElement(pmt_tp_inf, \"CtgyPurp\")\n",
    "ET.SubElement(ctgy_purp, \"Cd\").text = \"SALA\"\n",
    "\n",
    "\n",
    "# Current date formatted as YYYY-MM-DD\n",
    "current_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Dynamically assign it to your XML element\n",
    "ET.SubElement(pmt_inf, \"ReqdExctnDt\").text = current_date\n",
    "\n",
    "# Debtor Information\n",
    "dbtr = ET.SubElement(pmt_inf, \"Dbtr\")\n",
    "ET.SubElement(dbtr, \"Nm\").text = \"3WAQUISECONDUZ UNIPESSOAL LDA\"\n",
    "\n",
    "dbtr_acct = ET.SubElement(pmt_inf, \"DbtrAcct\")\n",
    "dbtr_acct_id = ET.SubElement(dbtr_acct, \"Id\")\n",
    "ET.SubElement(dbtr_acct_id, \"IBAN\").text = \"PT50003300004565690070705\"\n",
    "\n",
    "# Adding Debtor Agent information\n",
    "dbtr_agt = ET.SubElement(pmt_inf, \"DbtrAgt\")\n",
    "ET.SubElement(dbtr_agt, \"FinInstnId\").text = \"\"\n",
    "\n",
    "# Loop through payments_sem_iban DataFrame\n",
    "for row in payments_sem_iban.itertuples(index=False):\n",
    "    cdt_trf_tx_inf = ET.SubElement(pmt_inf, \"CdtTrfTxInf\")\n",
    "\n",
    "    pmt_id = ET.SubElement(cdt_trf_tx_inf, \"PmtId\")\n",
    "    ET.SubElement(pmt_id, \"EndToEndId\").text = row.Payment_ID if pd.notna(row.Payment_ID) else \"PAGAMENTO\"\n",
    "\n",
    "    amt = ET.SubElement(cdt_trf_tx_inf, \"Amt\")\n",
    "    ET.SubElement(amt, \"InstdAmt\", Ccy=\"EUR\").text = f\"{row.Amount:.2f}\"\n",
    "\n",
    "    cdtr_agt = ET.SubElement(cdt_trf_tx_inf, \"CdtrAgt\")\n",
    "    fin_instn_id = ET.SubElement(cdtr_agt, \"FinInstnId\")\n",
    "    fin_instn_id.text = \"\" \n",
    "\n",
    "    cdtr = ET.SubElement(cdt_trf_tx_inf, \"Cdtr\")\n",
    "    ET.SubElement(cdtr, \"Nm\").text = row.Creditor\n",
    "\n",
    "    cdtr_acct = ET.SubElement(cdt_trf_tx_inf, \"CdtrAcct\")\n",
    "    cdtr_acct_id = ET.SubElement(cdtr_acct, \"Id\")\n",
    "    ET.SubElement(cdtr_acct_id, \"IBAN\").text = row.IBAN\n",
    "\n",
    "## Converter data para yyyy-mm-dd\n",
    "new_data = datetime.strptime(data_arquivo, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Converte o XML em string\n",
    "xml_str = ET.tostring(root, encoding='utf-8')\n",
    "parsed = minidom.parseString(xml_str)\n",
    "pretty_xml = parsed.toprettyxml(indent=\"    \")  # indentação de 4 espaços\n",
    "\n",
    "# Corrige a primeira linha (declaração XML)\n",
    "pretty_xml = pretty_xml.replace('<?xml version=\"1.0\" ?>', '<?xml version=\"1.0\" encoding=\"utf-8\"?>')\n",
    "\n",
    "# Divide o XML formatado em linhas\n",
    "lines = pretty_xml.splitlines()\n",
    "\n",
    "# Junta os atributos da tag <Document> em uma única linha\n",
    "for i, line in enumerate(lines):\n",
    "    if '<Document' in line:\n",
    "        while not lines[i].strip().endswith('>'):\n",
    "            lines[i] = lines[i].strip() + ' ' + lines[i + 1].strip()\n",
    "            del lines[i + 1]\n",
    "        break\n",
    "\n",
    "# Mantém a primeira linha (declaração XML) sem indentação\n",
    "final_lines = [lines[0]]  # Primeira linha intacta\n",
    "\n",
    "# Aplica 12 espaços nas demais linhas (exceto linhas vazias)\n",
    "for line in lines[1:]:\n",
    "    final_lines.append(' ' * 12 + line if line.strip() else '')\n",
    "\n",
    "# Recompõe o XML final\n",
    "final_xml = '\\n'.join(final_lines)\n",
    "\n",
    "# Nome do arquivo XML\n",
    "sem_iban_batch_temp = f\"BATCHSEMIBAN.xml\"\n",
    "sem_iban_batch = os.path.join(new_folder_path, sem_iban_batch_temp)\n",
    "\n",
    "# Salva no arquivo\n",
    "with open(sem_iban_batch, 'w', encoding='utf-8') as f:\n",
    "    f.write(final_xml)\n",
    "\n",
    "print(f\"XML file generated: {sem_iban_batch_temp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78af6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML file generated: BATCHPROMOCAO.xml\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Nome do arquivo XML\n",
    "promo_xml_temp = \"PLANILHA PROMOCOES_MODELO-FINAL.xlsx\"\n",
    "promo_xml = os.path.join(new_folder_path, promo_xml_temp)\n",
    "\n",
    "# Load data\n",
    "payments_promo = pd.read_excel(promo_xml, sheet_name='XML')\n",
    "\n",
    "# Keep specific columns\n",
    "col_seguimentos_keep = [\"Novo Nome\", \"Valor a Pagar\", \"IBAN\", \"Arq_origem\"]\n",
    "payments_promo = payments_promo[col_seguimentos_keep]\n",
    "\n",
    "# Rename columns for consistency\n",
    "payments_promo.columns = [\"Creditor\", \"Amount\", \"IBAN\", \"Payment_ID\"]\n",
    "\n",
    "# Convert 'Creditor' to uppercase and slice\n",
    "payments_promo[\"Creditor\"] = payments_promo[\"Creditor\"].str.upper().str.slice(0, 28)\n",
    "\n",
    "# Clean 'IBAN' column (keep only letters and numbers, remove special characters)\n",
    "payments_promo[\"IBAN\"] = payments_promo[\"IBAN\"].fillna('0')\n",
    "payments_promo[\"IBAN\"] = payments_promo[\"IBAN\"].astype(str).str.replace(r'[^A-Za-z0-9]', '', regex=True)\n",
    "payments_promo[\"IBAN\"] = payments_promo[\"IBAN\"].str.upper()\n",
    "\n",
    "# Define a function to check IBAN rules\n",
    "def iban_check(iban):\n",
    "    if pd.isna(iban) or iban == '':\n",
    "        return False\n",
    "    first_char = iban[0]\n",
    "    if first_char.isalpha():\n",
    "        if first_char == 'P':\n",
    "            return len(iban) == 25\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Apply the function to filter correct IBANs\n",
    "payments_promo_validos = payments_promo[payments_promo['IBAN'].apply(iban_check)]\n",
    "\n",
    "# Apply the function to filter invalid IBANs\n",
    "payments_promo_nao_enviados = payments_promo[~payments_promo['IBAN'].apply(iban_check)]\n",
    "\n",
    "# Save invalid IBANs\n",
    "save_df_to_excel(payments_promo_nao_enviados, f\"payments_promo_nao_enviados.xlsx\", 'ao banco')\n",
    "\n",
    "# Continue with valid payments\n",
    "payments_promo_validos[\"Amount\"] = payments_promo_validos[\"Amount\"].astype(float)\n",
    "payments_promo_validos[\"Payment_ID\"] = \"PAGAMENTO SEMANAL\"\n",
    "\n",
    "# Save valid payments\n",
    "save_df_to_excel(payments_promo_validos, f\"payments_promo.xlsx\", 'ao banco')\n",
    "\n",
    "payments_promo = payments_promo_validos\n",
    "\n",
    "\n",
    "# XML structure\n",
    "root = ET.Element(\"Document\", {\n",
    "    \"xmlns\": \"urn:iso:std:iso:20022:tech:xsd:pain.001.001.03\",\n",
    "    \"xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\"\n",
    "})\n",
    "cstmr_cdt_trf_initn = ET.SubElement(root, \"CstmrCdtTrfInitn\")\n",
    "\n",
    "# Group Header\n",
    "grp_hdr = ET.SubElement(cstmr_cdt_trf_initn, \"GrpHdr\")\n",
    "\n",
    "# Generate timestamp in format YYYYMMDDHHMM\n",
    "timestamp = datetime.now().strftime('%Y%m%d%H%M')\n",
    "# Dynamically assign it to your XML element\n",
    "ET.SubElement(grp_hdr, \"MsgId\").text = f\"UPROFIT911B-{timestamp}\" # ATUALIZADO\n",
    "#ET.SubElement(grp_hdr, \"MsgId\").text = \"UPROFIT911B-2025012111123062\"\n",
    "\n",
    "ET.SubElement(grp_hdr, \"CreDtTm\").text = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "ET.SubElement(grp_hdr, \"NbOfTxs\").text = str(len(payments_promo))\n",
    "ET.SubElement(grp_hdr, \"CtrlSum\").text = f\"{payments_promo['Amount'].sum():.2f}\"\n",
    "\n",
    "initg_pty = ET.SubElement(grp_hdr, \"InitgPty\")\n",
    "ET.SubElement(initg_pty, \"Nm\").text = \"3WAQUISECONDUZ UNIPESSOAL LDA\"\n",
    "\n",
    "# Adding ID structure\n",
    "initg_pty_id = ET.SubElement(initg_pty, \"Id\")\n",
    "prvt_id = ET.SubElement(initg_pty_id, \"PrvtId\")\n",
    "othr = ET.SubElement(prvt_id, \"Othr\")\n",
    "ET.SubElement(othr, \"Id\").text = \"516574361\"\n",
    "\n",
    "# Payment Information\n",
    "pmt_inf = ET.SubElement(cstmr_cdt_trf_initn, \"PmtInf\")\n",
    "\n",
    "\n",
    "ET.SubElement(pmt_inf, \"PmtInfId\").text = f\"UPROFIT911B-{timestamp}\"\n",
    "#ET.SubElement(pmt_inf, \"PmtInfId\").text = \"UPROFIT911B-20250121111230\"\n",
    "\n",
    "ET.SubElement(pmt_inf, \"PmtMtd\").text = \"TRF\"\n",
    "ET.SubElement(pmt_inf, \"NbOfTxs\").text = str(len(payments_promo))\n",
    "ET.SubElement(pmt_inf, \"CtrlSum\").text = f\"{payments_promo['Amount'].sum():.2f}\"\n",
    "\n",
    "pmt_tp_inf = ET.SubElement(pmt_inf, \"PmtTpInf\")\n",
    "\n",
    "# IMEDIATA TAG\n",
    "lcl_instrm = ET.SubElement(pmt_tp_inf, \"LclInstrm\")\n",
    "ET.SubElement(lcl_instrm, \"Prtry\").text = \"INST\"\n",
    "\n",
    "ctgy_purp = ET.SubElement(pmt_tp_inf, \"CtgyPurp\")\n",
    "ET.SubElement(ctgy_purp, \"Cd\").text = \"SALA\"\n",
    "\n",
    "\n",
    "# Current date formatted as YYYY-MM-DD\n",
    "current_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Dynamically assign it to your XML element\n",
    "ET.SubElement(pmt_inf, \"ReqdExctnDt\").text = current_date\n",
    "\n",
    "# Debtor Information\n",
    "dbtr = ET.SubElement(pmt_inf, \"Dbtr\")\n",
    "ET.SubElement(dbtr, \"Nm\").text = \"3WAQUISECONDUZ UNIPESSOAL LDA\"\n",
    "\n",
    "dbtr_acct = ET.SubElement(pmt_inf, \"DbtrAcct\")\n",
    "dbtr_acct_id = ET.SubElement(dbtr_acct, \"Id\")\n",
    "ET.SubElement(dbtr_acct_id, \"IBAN\").text = \"PT50003300004565690070705\"\n",
    "\n",
    "# Adding Debtor Agent information\n",
    "dbtr_agt = ET.SubElement(pmt_inf, \"DbtrAgt\")\n",
    "ET.SubElement(dbtr_agt, \"FinInstnId\").text = \"\"\n",
    "\n",
    "# Loop through payments_sem_iban DataFrame\n",
    "for row in payments_promo.itertuples(index=False):\n",
    "    cdt_trf_tx_inf = ET.SubElement(pmt_inf, \"CdtTrfTxInf\")\n",
    "\n",
    "    pmt_id = ET.SubElement(cdt_trf_tx_inf, \"PmtId\")\n",
    "    ET.SubElement(pmt_id, \"EndToEndId\").text = row.Payment_ID if pd.notna(row.Payment_ID) else \"PAGAMENTO\"\n",
    "\n",
    "    amt = ET.SubElement(cdt_trf_tx_inf, \"Amt\")\n",
    "    ET.SubElement(amt, \"InstdAmt\", Ccy=\"EUR\").text = f\"{row.Amount:.2f}\"\n",
    "\n",
    "    cdtr_agt = ET.SubElement(cdt_trf_tx_inf, \"CdtrAgt\")\n",
    "    fin_instn_id = ET.SubElement(cdtr_agt, \"FinInstnId\")\n",
    "    fin_instn_id.text = \"\" \n",
    "\n",
    "    cdtr = ET.SubElement(cdt_trf_tx_inf, \"Cdtr\")\n",
    "    ET.SubElement(cdtr, \"Nm\").text = row.Creditor\n",
    "\n",
    "    cdtr_acct = ET.SubElement(cdt_trf_tx_inf, \"CdtrAcct\")\n",
    "    cdtr_acct_id = ET.SubElement(cdtr_acct, \"Id\")\n",
    "    ET.SubElement(cdtr_acct_id, \"IBAN\").text = row.IBAN\n",
    "\n",
    "## Converter data para yyyy-mm-dd\n",
    "new_data = datetime.strptime(data_arquivo, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Converte o XML em string\n",
    "xml_str = ET.tostring(root, encoding='utf-8')\n",
    "parsed = minidom.parseString(xml_str)\n",
    "pretty_xml = parsed.toprettyxml(indent=\"    \")  # indentação de 4 espaços\n",
    "\n",
    "# Corrige a primeira linha (declaração XML)\n",
    "pretty_xml = pretty_xml.replace('<?xml version=\"1.0\" ?>', '<?xml version=\"1.0\" encoding=\"utf-8\"?>')\n",
    "\n",
    "# Divide o XML formatado em linhas\n",
    "lines = pretty_xml.splitlines()\n",
    "\n",
    "# Junta os atributos da tag <Document> em uma única linha\n",
    "for i, line in enumerate(lines):\n",
    "    if '<Document' in line:\n",
    "        while not lines[i].strip().endswith('>'):\n",
    "            lines[i] = lines[i].strip() + ' ' + lines[i + 1].strip()\n",
    "            del lines[i + 1]\n",
    "        break\n",
    "\n",
    "# Mantém a primeira linha (declaração XML) sem indentação\n",
    "final_lines = [lines[0]]  # Primeira linha intacta\n",
    "\n",
    "# Aplica 12 espaços nas demais linhas (exceto linhas vazias)\n",
    "for line in lines[1:]:\n",
    "    final_lines.append(' ' * 12 + line if line.strip() else '')\n",
    "\n",
    "# Recompõe o XML final\n",
    "final_xml = '\\n'.join(final_lines)\n",
    "\n",
    "# Nome do arquivo XML\n",
    "promo_batch_temp = f\"BATCHPROMOCAO.xml\"\n",
    "promo_batch = os.path.join(new_folder_path, promo_batch_temp)\n",
    "\n",
    "# Salva no arquivo\n",
    "with open(promo_batch, 'w', encoding='utf-8') as f:\n",
    "    f.write(final_xml)\n",
    "\n",
    "print(f\"XML file generated: {promo_batch_temp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cd69eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd47705a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656d76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433b716f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
